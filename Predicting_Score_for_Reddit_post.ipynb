{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting Score for Reddit post.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJCoaPxEntbm",
        "outputId": "7c2591c2-55e5-4818-8f0a-9dac9162cba0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxChx38YJjOP"
      },
      "source": [
        "from numpy import array\r\n",
        "from numpy import asarray\r\n",
        "from numpy import zeros\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import numpy as np\r\n",
        "from collections import deque\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-RDvzC7NaM3",
        "outputId": "e20b16e9-b856-45ee-f5ff-ab9f8551013e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2w5hnIGJrJS"
      },
      "source": [
        "data = pd.read_csv('FinalData.csv')\r\n",
        "data.drop(['Unnamed: 0','Upvote_ratio'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duFv27Q3IlnF"
      },
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\r\n",
        "import re\r\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\r\n",
        "\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "stemmer = PorterStemmer()\r\n",
        "def text_preprocess(text):\r\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) \r\n",
        "    l_text = [word for word in text.lower().split() if word not in ENGLISH_STOP_WORDS]\r\n",
        "    stem_words = [stemmer.stem(w) for w in l_text]\r\n",
        "    lemma_words = [lemmatizer.lemmatize(w) for w in l_text]\r\n",
        "    return \" \".join(lemma_words)\r\n",
        "data['Title'] = data['Title'].map(lambda com : text_preprocess(com))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEKIbA-ccGpj"
      },
      "source": [
        "i = 0\r\n",
        "predicted_value = []\r\n",
        "while i<len(data):\r\n",
        "  if (data.loc[i]['compound'] >= 0.5):\r\n",
        "    predicted_value.append('positive')\r\n",
        "    i = i+1\r\n",
        "\r\n",
        "  elif (data.loc[i]['compound'] >= 0) & (data.loc[i]['compound'] <= 0.5):\r\n",
        "    predicted_value.append('neutral')\r\n",
        "    i = i+1\r\n",
        "\r\n",
        "  elif (data.loc[i]['compound'] <= 0):\r\n",
        "    predicted_value.append('negative')\r\n",
        "    i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BApFtMYfOUb"
      },
      "source": [
        "data['Predicted_value'] = predicted_value\r\n",
        "data.drop(['neg', 'neu', 'pos', 'compound'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYdi5jvmPCxC"
      },
      "source": [
        "X = data.drop(['Score'], axis=1)\r\n",
        "y = data['Score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjN1MgOvKJ-O"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TonbD7jNKLgA"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\r\n",
        "tokenizer.fit_on_texts(X_train[\"Title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_EVgqi4gv0Y"
      },
      "source": [
        "train_title = tokenizer.texts_to_sequences(X_train[\"Title\"])\r\n",
        "test_title = tokenizer.texts_to_sequences(X_test[\"Title\"])\r\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5Gf7sikIDOm"
      },
      "source": [
        "indixes = tokenizer.word_index\r\n",
        "tokens = {k:[indixes[k]] for k in indixes}\r\n",
        "df_tokens = pd.DataFrame(tokens)\r\n",
        "df_tokens.to_csv('tokenizer.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O39OzsugKO0_"
      },
      "source": [
        "maxlen = 300\r\n",
        "train_title = pad_sequences(train_title, padding='post', maxlen=maxlen)\r\n",
        "test_title = pad_sequences(test_title, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpVFhOlfKQzS"
      },
      "source": [
        "embeddings_dictionary = dict()\r\n",
        "glove_file = open('/content/drive/My Drive/glove.6B.100d.txt', encoding=\"utf8\")\r\n",
        "for line in glove_file:\r\n",
        "    records = line.split()\r\n",
        "    word = records[0]\r\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\r\n",
        "    embeddings_dictionary[word] = vector_dimensions\r\n",
        "glove_file.close()\r\n",
        "\r\n",
        "embedding_matrix = zeros((vocab_size, 100))\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    embedding_vector = embeddings_dictionary.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ObLLfBLKGkq"
      },
      "source": [
        "df = pd.DataFrame(data=embedding_matrix.astype(float))\r\n",
        "df.to_csv('glove.csv', sep=' ', header=True, float_format='%.2f', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLOjS3kqLLzi"
      },
      "source": [
        "train_new = {}\r\n",
        "for i, sentence in enumerate(train_title):\r\n",
        "    vectors = []\r\n",
        "    for n in sentence:\r\n",
        "        vectors.append(embedding_matrix[n])\r\n",
        "    train_new[i] = vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6j6KzGrLN80"
      },
      "source": [
        "test_new = {}\r\n",
        "for i, sentence in enumerate(test_title):\r\n",
        "    vectors = []\r\n",
        "    for n in sentence:\r\n",
        "        vectors.append(embedding_matrix[n])\r\n",
        "    test_new[i] = vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2P2SK27LQRI"
      },
      "source": [
        "X_train_df = []\r\n",
        "for key in train_new:\r\n",
        "    arr = np.array(train_new[key])\r\n",
        "    X_train_df.append(np.mean(arr, axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Qc2kiuLSUA"
      },
      "source": [
        "X_test_df = []\r\n",
        "for key in test_new:\r\n",
        "    arr = np.array(test_new[key])\r\n",
        "    X_test_df.append(np.mean(arr, axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATHo8IXPLUk1"
      },
      "source": [
        "X_train_df = pd.DataFrame(np.array(X_train_df))\r\n",
        "X_test_df = pd.DataFrame(np.array(X_test_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvyHRjxUYNVA"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "categories = ['Over_18', 'Predicted_value']\r\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\r\n",
        "enc.fit(X_train[categories])\r\n",
        "pickle.dump(enc, open('encoding.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDd0KPZAdfLg"
      },
      "source": [
        "col_names = [j for sub in enc.categories_ for j in sub] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWYga4-bY_iG"
      },
      "source": [
        "train_encoded = enc.transform(X_train[categories])\r\n",
        "test_encoded = enc.transform(X_test[categories])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBajHnZ-LXE-",
        "outputId": "e42b204a-1eeb-4b55-9d17-e6f25690830c"
      },
      "source": [
        "X_train.drop([\"Title\", 'Over_18', 'Predicted_value'], axis=1, inplace=True)\r\n",
        "X_test.drop([\"Title\", 'Over_18', 'Predicted_value'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QOGMu8rLaIa"
      },
      "source": [
        "X_train.reset_index(inplace=True, drop=True)\r\n",
        "X_test.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLv5i964c8RL"
      },
      "source": [
        "train = pd.DataFrame(train_encoded.todense(), columns=col_names)\r\n",
        "test = pd.DataFrame(test_encoded.todense(), columns=col_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSw1wZgBLcaI"
      },
      "source": [
        "X_train = pd.concat([X_train, X_train_df, train], axis=1)\r\n",
        "X_test = pd.concat([X_test, X_test_df, test], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00QRjSdaVAtB"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score, mean_absolute_error\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.neighbors import KNeighborsRegressor\r\n",
        "import xgboost as xgb\r\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dljkqiBnMtlO",
        "outputId": "cbfc09eb-2570-4ec3-e6ca-48e219253339"
      },
      "source": [
        "knn=KNeighborsRegressor()\r\n",
        "knn.fit(X_train,y_train)\r\n",
        "test_preds2=knn.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds2)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',knn.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  7451.644931659574\n",
            "RSquared value on test: 0.36109834692091725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-3yec1JL8Tw",
        "outputId": "bf4e913f-7ea1-42d4-d4f6-46c404a5eed4"
      },
      "source": [
        "lm=LinearRegression()   \r\n",
        "lm = lm.fit(X_train,y_train)\r\n",
        "test_pred = lm.predict(X_test)\r\n",
        "RMSE_test = np.sqrt(mean_squared_error(y_test, test_pred))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',lm.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  8948.74957123513\n",
            "RSquared value on test: 0.07858682970448438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vDeq1jkL15F",
        "outputId": "754386ff-e614-4ff0-cf10-25ea52d58b80"
      },
      "source": [
        "DT=DecisionTreeRegressor()\r\n",
        "DT.fit(X_train,y_train)\r\n",
        "test_preds=DT.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',DT.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  9989.052566903001\n",
            "RSquared value on test: -0.14809626350053007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75jGUB_XMiZg",
        "outputId": "3970393b-251f-45a4-9339-7c41fb437a65"
      },
      "source": [
        "RF=RandomForestRegressor(n_jobs=-1)\r\n",
        "RF.fit(X_train,y_train)\r\n",
        "test_preds1=RF.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds1)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',RF.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  6810.250588597596\n",
            "RSquared value on test: 0.46635076500393124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtBu8lhW3tKc"
      },
      "source": [
        "knn=KNeighborsRegressor()\r\n",
        "knn.fit(X_train,y_train)\r\n",
        "pickle.dump(knn, open('kn.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3MrZDxSNuXn"
      },
      "source": [
        "from sklearn.linear_model import LassoCV\r\n",
        "from sklearn.linear_model import RidgeCV\r\n",
        "from sklearn.linear_model import ElasticNetCV\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq38GaC3mSzQ",
        "outputId": "84acec0b-df34-4ca0-e2fa-aa048edae68c"
      },
      "source": [
        "xgbr =xgb.XGBRegressor().fit(X_train, y_train)\r\n",
        "test_preds6=xgbr.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds6)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',xgbr.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10:07:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "RMSE TestData =  6559.128029755439\n",
            "RSquared value on test: 0.5049809383493384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65cOItTjjz8u",
        "outputId": "2ada75ce-11fb-4a1a-9d7c-61bfc82728b2"
      },
      "source": [
        "lasso = LassoCV(cv=10).fit(X_train, y_train)\r\n",
        "test_preds3=lasso.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds3)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',lasso.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  9049.800344322846\n",
            "RSquared value on test: 0.05765983503903849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EKyaYHsj92x",
        "outputId": "cdc9cc41-d480-4f64-b537-2d0ea0137c5e"
      },
      "source": [
        "ridge = RidgeCV(cv=10).fit(X_train, y_train)\r\n",
        "test_preds4=ridge.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds4)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',ridge.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  8939.214392384392\n",
            "RSquared value on test: 0.08054937397011164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIrf-Wp9k07f",
        "outputId": "d1c25bd3-4d49-4431-830e-09061417dc5f"
      },
      "source": [
        "elastic_net = ElasticNetCV(cv = 10).fit(X_train, y_train)\r\n",
        "test_preds5=elastic_net.predict(X_test)\r\n",
        "RMSE_test=(np.sqrt(metrics.mean_squared_error(y_test,test_preds5)))\r\n",
        "print(\"RMSE TestData = \",str(RMSE_test))\r\n",
        "print('RSquared value on test:',elastic_net.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE TestData =  9002.04898892087\n",
            "RSquared value on test: 0.06757813205862173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8985qHJriUh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}